{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer # BoW vectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the \"archive.zip\" file\n",
    "archive_path = './archive.zip'\n",
    "\n",
    "# Extract the data from the ZIP file\n",
    "with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('extracted_data')\n",
    "    \n",
    "\n",
    "# Initialize an empty set for the vocabulary\n",
    "vocabulary = set()\n",
    "\n",
    "# Define a translation table to remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Recursive function to process files in a directory\n",
    "def process_directory(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                # Convert text to lowercase, remove punctuation, and split into words\n",
    "                words = content.lower().translate(translator).split()\n",
    "                words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "                # Add unique words to the vocabulary set\n",
    "                vocabulary.update(words)\n",
    "\n",
    "# Process the data directory to build the vocabulary\n",
    "process_directory('extracted_data')\n",
    "\n",
    "# Convert the vocabulary set into a dictionary that maps words to indices\n",
    "vocabulary_dict = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33494\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Bernaulli and Poisson distributions\n",
    "# Load and preprocess the data while mapping words to their indices\n",
    "def load_and_preprocess_data(data_path, vocabulary_dict):\n",
    "    data_bern = []\n",
    "    data_pois = []\n",
    "    data_multi = []\n",
    "    labels = []\n",
    "\n",
    "    newsgroup_folders = os.listdir(data_path)\n",
    "    label_to_index = {folder: i for i, folder in enumerate(newsgroup_folders)}\n",
    "\n",
    "    for folder in newsgroup_folders:\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with open(file_path, 'r', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                # Tokenize the content into words and convert to lowercase\n",
    "                words = content.lower().translate(translator).split()\n",
    "                words = [word for word in words if word.lower() not in stop_words]\n",
    "                data_curr = np.full(len(vocabulary), 0)\n",
    "                data_currp = np.full(len(vocabulary), 0)\n",
    "                data_currm = np.full(len(words), -1)\n",
    "                for j, word in enumerate(words):\n",
    "                    i = vocabulary_dict.get(word, -1)\n",
    "                    data_currm[j] = i\n",
    "                    # print(i)\n",
    "                    if i != -1 :\n",
    "                        data_curr[i] += 1\n",
    "                        data_currp[i] = 1\n",
    "                data_bern.append(data_curr)\n",
    "                data_pois.append(data_currp)\n",
    "                data_multi.append(data_currm)\n",
    "                labels.append(label_to_index[folder])\n",
    "\n",
    "    return data_bern, data_pois, data_multi, labels\n",
    "\n",
    "# Load and preprocess the data while mapping words to their indices using vocabulary_dict\n",
    "data_bern, data_pois, data_multi, labels = load_and_preprocess_data('extracted_data', vocabulary_dict)\n",
    "\n",
    "\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(data_bern, labels, test_size=0.2)\n",
    "Xp_train, Xp_test, yp_train, yp_test = train_test_split(data_pois, labels, test_size=0.2)\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(data_multi, labels, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_pois)\n",
    "# print(len(data_bern[0]))\n",
    "# # Count the number of 1s in data[0]\n",
    "# def count_ones(i):\n",
    "#     return np.count_nonzero(data_pois[i] == 0)\n",
    "# print(f\"Number of nonzeroes in datapois[0]: {len(data_pois[2])} - {count_ones(2)} = {len(data_pois[2]) - count_ones(2)}\")\n",
    "# print(f\"Number of nonzeroes in datapois[0]: {len(data_pois[3])} - {count_ones(3)} = {len(data_pois[3]) - count_ones(3)}\")\n",
    "# print(f\"Number of nonzeroes in datapois[0]: {len(data_pois[4])} - {count_ones(4)} = {len(data_pois[4]) - count_ones(4)}\")\n",
    "# print(f\"Number of nonzeroes in datapois[0]: {len(data_pois[5])} - {count_ones(5)} = {len(data_pois[5]) - count_ones(5)}\")\n",
    "# print(len(data_multi[0]))\n",
    "# print(len(data_multi[1]))\n",
    "# print(len(data_multi[2]))\n",
    "# print(len(data_multi[3]))\n",
    "# count_ones = np.count_nonzero(data_multi[1] == 0)\n",
    "# print(f\"Number of nonzeroes in datamulti[0]: {len(data_multi[1])} - {count_ones} = {len(data_multi[1]) - count_ones}\")\n",
    "# # print(Xb_train, yb_train)\n",
    "\n",
    "# print(Xm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self, distribution='multinomial'):\n",
    "        self.distribution = distribution\n",
    "        self.class_probs = {}\n",
    "        self.feature_probs = {}\n",
    "    \n",
    "    def train(self, X, y, alpha=1):\n",
    "        \n",
    "        # Calculate class probabilities\n",
    "        total_samples = len(y)\n",
    "        unique_classes = np.unique(y)\n",
    "        for class_label in unique_classes:\n",
    "            class_count = np.sum(y == class_label)\n",
    "            self.class_probs[class_label] = class_count / total_samples\n",
    "    \n",
    "            # Calculate feature probabilities based on distribution\n",
    "            if self.distribution == 'bernoulli':\n",
    "                theta_j = []\n",
    "                for j in range(len(vocabulary_dict)):\n",
    "                    numerator = 0\n",
    "                    for i in range(len(X)):\n",
    "                        if y[i] == class_label:\n",
    "                            numerator += X[i][j]\n",
    "                    theta_j.append((numerator + 1*alpha) / (class_count + 33000*alpha))  \n",
    "                self.feature_probs[class_label] = theta_j\n",
    "                \n",
    "            elif self.distribution == 'poisson':\n",
    "                lambda_j = []\n",
    "                for j in range(len(vocabulary_dict)):\n",
    "                    numerator = 0\n",
    "                    for i in range(len(X)):\n",
    "                        if y[i] == class_label:\n",
    "                            numerator += X[i][j]\n",
    "                    lambda_j.append((numerator + 1*alpha) / (class_count + 33000*alpha))  \n",
    "                self.feature_probs[class_label] = lambda_j\n",
    "                \n",
    "            elif self.distribution == 'multinomial':\n",
    "                theta_jl = []\n",
    "                for l in range(len(vocabulary_dict)):  # Assuming you have a vocabulary_dict\n",
    "                    numerator = 0\n",
    "                    denominator = 0\n",
    "                    \n",
    "                    for i in range(len(X)):\n",
    "                        if y[i] == class_label:\n",
    "                            # Count occurrences of the word in documents of this class\n",
    "                            word_count = np.sum(X[i] == l)\n",
    "                            numerator += word_count\n",
    "                            denominator += len(X[i])\n",
    "\n",
    "                    # Apply Laplace smoothing to avoid zero probabilities\n",
    "                    smoothed_prob = (numerator + 1*alpha) / (denominator + (len(vocabulary_dict) * alpha))\n",
    "                    theta_jl.append(smoothed_prob)\n",
    "                \n",
    "                self.feature_probs[class_label] = theta_jl\n",
    "          \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        if self.distribution == 'bernoulli':\n",
    "            for sample in X:\n",
    "                # Calculate class probabilities using feature probabilities and class probabilities\n",
    "                class_scores = {}  # Dictionary to store scores for each class\n",
    "                for class_label, class_prob in self.class_probs.items():\n",
    "                    score = np.log(class_prob)  # Initialize with class probability\n",
    "                    for j, x_j in enumerate(sample):\n",
    "                        if x_j == 1:\n",
    "                            # Add the log-probability for feature j being 1\n",
    "                            score += np.log(self.feature_probs[class_label][j])\n",
    "                        else:\n",
    "                            # Add the log-probability for feature j being 0 (1 - the probability of being 1)\n",
    "                            score += np.log(1 - self.feature_probs[class_label][j])\n",
    "                    class_scores[class_label] = score\n",
    "\n",
    "                # Select the class with the highest score\n",
    "                predicted_class = max(class_scores, key=class_scores.get)\n",
    "                predictions.append(predicted_class)\n",
    "\n",
    "        elif self.distribution == 'poisson':\n",
    "            for sample in X:\n",
    "                # Calculate class probabilities using feature probabilities and class probabilities\n",
    "                class_scores = {}  # Dictionary to store scores for each class\n",
    "                for class_label, class_prob in self.class_probs.items():\n",
    "                    score = np.log(class_prob)  # Initialize with class probability\n",
    "                    for j, x_j in enumerate(sample):\n",
    "                        # Calculate the Poisson probability for each feature j\n",
    "                        # Assuming self.feature_probs stores Î» values\n",
    "                        poisson_prob = np.exp(-self.feature_probs[class_label][j]) * (self.feature_probs[class_label][j] ** x_j) / np.math.factorial(x_j)\n",
    "                        score += np.log(poisson_prob)\n",
    "                    class_scores[class_label] = score\n",
    "\n",
    "                # Select the class with the highest score\n",
    "                predicted_class = max(class_scores, key=class_scores.get)\n",
    "                predictions.append(predicted_class)\n",
    "\n",
    "        elif self.distribution == 'multinomial':\n",
    "            for sample in X:\n",
    "                # Calculate class probabilities using feature probabilities and class probabilities\n",
    "                class_scores = {}  # Dictionary to store scores for each class\n",
    "                for class_label, class_prob in self.class_probs.items():\n",
    "                    score = np.log(class_prob)  # Initialize with class probability\n",
    "                    for j, x_j in enumerate(sample):\n",
    "                        # Assuming self.feature_probs stores word probabilities for each class\n",
    "                        word_prob = self.feature_probs[class_label][x_j]\n",
    "                        score += np.log(word_prob)\n",
    "                    class_scores[class_label] = score\n",
    "\n",
    "                # Select the class with the highest score\n",
    "                predicted_class = max(class_scores, key=class_scores.get)\n",
    "                predictions.append(predicted_class)\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_b = NaiveBayes(distribution='bernoulli',)\n",
    "nb_classifier_b.train(Xb_train, yb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nb_classifier_b.class_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nb_classifier_b.feature_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_b = nb_classifier_b.predict(Xb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Bernoulli Disribution: 0.815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_b = np.array(predictions_b)\n",
    "yb_test = np.array(yb_test)\n",
    "# Assuming y_pred and y_actual are NumPy arrays or lists\n",
    "accuracy = np.mean(predictions_b == yb_test)\n",
    "print(f\"Accuracy for Bernoulli Disribution: {accuracy}\")\n",
    "# predictions_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_p = NaiveBayes(distribution='poisson')\n",
    "nb_classifier_p.train(Xp_train, yp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nb_classifier_p.class_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nb_classifier_p.feature_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashut\\AppData\\Local\\Temp\\ipykernel_26124\\3868215889.py:89: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  poisson_prob = np.exp(-self.feature_probs[class_label][j]) * (self.feature_probs[class_label][j] ** x_j) / np.math.factorial(x_j)\n"
     ]
    }
   ],
   "source": [
    "predictions_p = nb_classifier_p.predict(Xp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Accuracy for Poisson Distribution: 0.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_p = np.array(predictions_p)\n",
    "print(len(predictions_p))\n",
    "yp_test = np.array(yp_test)\n",
    "accuracy = np.mean(predictions_p == yp_test)\n",
    "print(f\"Accuracy for Poisson Distribution: {accuracy}\")\n",
    "# predictions_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_m = NaiveBayes(distribution='multinomial')\n",
    "nb_classifier_m.train(Xm_train, ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nb_classifier_m.class_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nb_classifier_m.feature_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_m = nb_classifier_m.predict(Xm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Multinoulli Distribution: 0.985\n"
     ]
    }
   ],
   "source": [
    "predictions_m = np.array(predictions_m)\n",
    "ym_test = np.array(ym_test)\n",
    "# Assuming y_pred and y_actual are NumPy arrays or lists\n",
    "accuracy = np.mean(predictions_m == ym_test)\n",
    "print(f\"Accuracy for Multinoulli Distribution: {accuracy}\")\n",
    "# predictions_m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
